{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=False)"],"metadata":{"id":"qaYbjSY3X1f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q python-dotenv"],"metadata":{"id":"wpjD1WO7C_DM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(\"/content/drive/MyDrive/secrets/.env\")"],"metadata":{"id":"oHx0_oFvUNBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **학습 데이터 생성**"],"metadata":{"id":"YZA5Nx2j9qRz"}},{"cell_type":"code","source":["import os, json, re, unicodedata, random\n","from pathlib import Path\n","import pandas as pd\n","\n","try:\n","    from google.colab import drive  # type: ignore\n","    if not Path(\"/content/drive\").exists():\n","        drive.mount(\"/content/drive\")\n","    else:\n","        pass\n","except Exception:\n","    pass\n","\n","ROOT = \"/content/drive/MyDrive/Summarize\"\n","INPUT_DIR = f\"{ROOT}/summaries\"\n","OUT_DIR   = f\"{ROOT}/dataset_chat\"\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","\n","print(\"ROOT       :\", ROOT)\n","print(\"INPUT_DIR  :\", INPUT_DIR)\n","print(\"OUT_DIR    :\", OUT_DIR)"],"metadata":{"id":"nUsocyWjFBBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from glob import glob\n","cleaned_files = sorted(glob(f\"{INPUT_DIR}/*_events_reviews_CLEANED.csv\"))\n","sum_files     = sorted(glob(f\"{INPUT_DIR}/*_events_reviews_CLEANED_gemini_summaries.csv\"))\n","\n","def _key(p: str) -> str:\n","    name = Path(p).name\n","    return name.split(\"_events_reviews_\")[0]\n","\n","cleaned_map = {_key(p): p for p in cleaned_files}\n","sum_map     = {_key(p): p for p in sum_files}\n","regions     = sorted(set(cleaned_map) & set(sum_map))\n","\n","print(f\"[pairs] matched = {len(regions)}\")\n","for k in regions:\n","    print(f\"  {k} → {Path(cleaned_map[k]).name} | {Path(sum_map[k]).name}\")\n","\n","# 각 파일 컬럼 검증 + 병합\n","full_parts = []\n","for k in regions:\n","    df_clean = pd.read_csv(cleaned_map[k])\n","    df_sum   = pd.read_csv(sum_map[k])\n","\n","    need_clean = {\"contentid\",\"event_title\",\"event_addr\",\"cleaned_review\"}\n","    need_sum   = {\"contentid\",\"event_title\",\"event_addr\",\"summary\"}\n","\n","    if not need_clean.issubset(df_clean.columns):\n","        raise ValueError(f\"{cleaned_map[k]} 에 필수 컬럼 누락: {need_clean - set(df_clean.columns)}\")\n","    if not need_sum.issubset(df_sum.columns):\n","        raise ValueError(f\"{sum_map[k]} 에 필수 컬럼 누락: {need_sum - set(df_sum.columns)}\")\n","\n","    # contentid 기준 내부 조인 (중복 contentid 있으면 정리)\n","    df_sum = df_sum.drop_duplicates(subset=[\"contentid\"], keep=\"first\")\n","    merged = pd.merge(\n","        df_clean[[\"contentid\",\"event_title\",\"event_addr\",\"cleaned_review\"]],\n","        df_sum[[\"contentid\",\"event_title\",\"event_addr\",\"summary\"]],\n","        on=\"contentid\",\n","        how=\"inner\",\n","        suffixes=(\"_clean\",\"_sum\")\n","    )\n","\n","    merged[\"event_title\"] = merged[\"event_title_sum\"].fillna(merged[\"event_title_clean\"]).astype(str)\n","    merged[\"event_addr\"]  = merged[\"event_addr_sum\"].fillna(merged[\"event_addr_clean\"]).astype(str)\n","    merged = merged[[\"contentid\",\"event_title\",\"event_addr\",\"cleaned_review\",\"summary\"]]\n","\n","    full_parts.append(merged)\n","\n","full = pd.concat(full_parts, ignore_index=True) if full_parts else pd.DataFrame(\n","    columns=[\"contentid\",\"event_title\",\"event_addr\",\"cleaned_review\",\"summary\"]\n",")\n","\n","print(\"merged rows:\", len(full))\n","full.head(3)"],"metadata":{"id":"eI5RsRdLFA-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re, unicodedata\n","\n","_WS   = re.compile(r\"\\s+\")\n","_HANG = re.compile(r\"[가-힣]\")\n","\n","R_HAN   = r\"\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\"\n","R_HIRA  = r\"\\u3040-\\u309F\"\n","R_KATA  = r\"\\u30A0-\\u30FF\"\n","R_HKATA = r\"\\uFF65-\\uFF9F\"\n","_CJK_OTHER = re.compile(rf\"[{R_HAN}{R_HIRA}{R_KATA}{R_HKATA}]\")\n","_CJK_SMALL_RUN = re.compile(rf\"(?<![가-힣])[{R_HAN}{R_HIRA}{R_KATA}{R_HKATA}]{{1,3}}(?![가-힣])\")\n","_CJK_LONG_RUN  = re.compile(rf\"[{R_HAN}{R_HIRA}{R_KATA}{R_HKATA}]{{4,}}\")\n","\n","_re_jp_cn_punct = re.compile(r\"[、，。；：「」『』・〜]\")\n","_re_trans_tail  = re.compile(r\"(?is)(?:^|\\n)\\s*(?:translation|translated by|번역)\\s*[:\\-].*$\")\n","_re_noise_token = re.compile(r\"(?:\\bnull\\b|@[A-Za-z0-9_]+)\")\n","_re_long_alpha  = re.compile(r\"\\b[A-Za-z]{20,}\\b\")\n","\n","_KOR_ENDINGS = (\"습니다\",\"합니다\",\"했습니다\",\"했다\",\"한다\",\"해요\",\"예요\",\"이에요\",\"다\")\n","_PUNCT_SPLIT_RE = re.compile(r\"([.!?])\\s+\")\n","_ENDINGS_ALT    = re.compile(r\"(?:%s)\\s+\" % \"|\".join(map(re.escape, _KOR_ENDINGS)))\n","\n","def han_ratio(s: str) -> float:\n","    s = unicodedata.normalize(\"NFKC\", s or \"\")\n","    vis = sum(not ch.isspace() for ch in s)\n","    if vis == 0: return 0.0\n","    han = sum(\"가\" <= ch <= \"힣\" for ch in s)\n","    return han / vis\n","\n","def sent_split_ko(text: str):\n","    if not isinstance(text, str): return []\n","    s = unicodedata.normalize(\"NFKC\", text)\n","    s = _WS.sub(\" \", s).strip()\n","    if not s: return []\n","    s = _re_trans_tail.sub(\"\", s)\n","    s = _PUNCT_SPLIT_RE.sub(r\"\\1<eos> \", s)\n","    s = _ENDINGS_ALT.sub(lambda m: m.group(0).rstrip() + \"<eos> \", s)\n","    parts = [p.strip() for p in s.split(\"<eos>\") if p.strip()]\n","    fixed = []\n","    for p in parts:\n","        if not re.search(r\"[.!?]$\", p) and not p.endswith(_KOR_ENDINGS):\n","            p += \".\"\n","        fixed.append(p)\n","    return fixed\n","\n","def _normalize_punct(s: str) -> str:\n","    s = (s or \"\")\n","    s = s.replace(\"、\", \", \").replace(\"。\", \". \")\n","    s = s.replace(\"「\", \"“\").replace(\"」\", \"”\")\n","    s = s.replace(\"\\uFF0C\", \", \").replace(\"\\uFF0E\", \". \").replace(\"\\u00A0\", \" \")\n","    s = s.replace(\"�\", \"\")\n","    return s\n","\n","def _strip_noise_tokens(s: str) -> str:\n","    toks = re.split(r\"(\\s+)\", s)\n","    out = []\n","    for tk in toks:\n","        raw = tk.strip()\n","        if not raw:\n","            out.append(tk); continue\n","        if _re_noise_token.search(raw):\n","            continue\n","        if _re_long_alpha.fullmatch(raw):\n","            continue\n","        out.append(tk)\n","    return \"\".join(out)\n","\n","def _squeeze_year_noise(s: str) -> str:\n","    if not s: return s\n","    t = s\n","    t = re.sub(r\"([가-힣])\\s*((?:19|20)\\d{2})\\s*(?=[가-힣])\", r\"\\1\", t)\n","    t = re.sub(r\"(?<!\\d)((?:19|20)\\d{2})\\d{1,3}(?=[^\\d]|$)\", r\"\\1\", t)\n","    t = re.sub(r\"(?<!\\d)\\d{5,}(?=[^\\d]|$)\", \"\", t)\n","    def _shrink(m):\n","        yrs = re.findall(r\"(?:19|20)\\d{2}\", m.group(0))\n","        return \", \".join(yrs[:2])\n","    t = re.sub(r\"\\b((?:19|20)\\d{2})(?:\\D+(?:19|20)\\d{2}){2,}\\b\", _shrink, t)\n","    t = re.sub(r\"(?<!\\d)(?:19|20)\\d{2}(?!\\s*[년월일])(?=[\\s가-힣\\.,)\\]]|$)\", \"\", t)\n","    return _WS.sub(\" \", t).strip()\n","\n","def _strip_nonko_cjk(s: str) -> str:\n","    \"\"\"한국어와 붙어있지 않은 비한글 CJK(한자/일문) 제거. 긴 런은 통째 제거.\"\"\"\n","    if not s: return s\n","    x = _CJK_LONG_RUN.sub(\"\", s)\n","    x = _CJK_SMALL_RUN.sub(\"\", x)\n","    x = _re_jp_cn_punct.sub(lambda m: \",\" if m.group(0) in \"、，\" else \".\", x)\n","    return _WS.sub(\" \", x).strip()\n","\n","def repair_summary(text: str, title: str = None, min_han_ratio: float = 0.70) -> str:\n","    \"\"\"타깃 요약을 학습에 적합하게 정리(한자/일문 제거 + 숫자폭주 억제 + 1~2문장).\"\"\"\n","    x = unicodedata.normalize(\"NFKC\", text or \"\")\n","    x = _re_trans_tail.sub(\"\", x)\n","    x = _normalize_punct(x)\n","    x = _strip_noise_tokens(x)\n","    x = _squeeze_year_noise(x)\n","    x = _strip_nonko_cjk(x)\n","\n","    if title:\n","        t = re.escape(title.strip())\n","        x = re.sub(rf\"^{t}\\s*/?\\s*\", \"\", x)\n","        if re.match(r\"^에서는\\b\", x):\n","            x = f\"{title.strip()} {x}\"\n","\n","    sents = sent_split_ko(x)\n","    sents = [s for s in sents if han_ratio(s) >= min_han_ratio]\n","    if not sents:\n","        sents = [s for s in sent_split_ko(x) if han_ratio(s) >= 0.60]\n","    sents = sents[:2] if sents else []\n","\n","    out = \" \".join(sents).strip()\n","    if out and not re.search(r\"[.!?]$\", out):\n","        out += \".\"\n","    return out\n","\n","def clean_review_to_korean(text: str, keep_thresh: float = 0.20, max_sents: int = 40) -> str:\n","    \"\"\"리뷰는 최대한 보존하되, 번역 꼬리/일본식 구두점만 정리.\"\"\"\n","    s = unicodedata.normalize(\"NFKC\", text or \"\")\n","    s = _re_trans_tail.sub(\"\", s)\n","    s = _re_jp_cn_punct.sub(lambda m: \",\" if m.group(0) in \"、，\" else \".\", s)\n","    s = _WS.sub(\" \", s).strip()\n","    parts = sent_split_ko(s)\n","    cleaned = []\n","    for p in parts:\n","        r = han_ratio(p)\n","        if r >= keep_thresh or _HANG.search(p) or _CJK_OTHER.search(p):\n","            cleaned.append(p)\n","    if not cleaned:\n","        return \"\"\n","    cleaned = cleaned[:max_sents]\n","    out = \" \".join(cleaned)\n","    return _WS.sub(\" \", out).strip()"],"metadata":{"id":"AVH_8dTbFA7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SYS_PROMPT = (\n","    \"당신은 여행 리뷰 데이터를 요약하는 어시스턴트입니다.\\n\"\n","    \"- 리뷰의 핵심 경험을 1~2문장으로 간결하게 정리합니다.\\n\"\n","    \"- 과장/광고 톤 없이 담백하게, 리뷰의 감정 뉘앙스를 자연스럽게 반영합니다.\\n\"\n","    \"- 구체 팩트 + 좋았던 점 1개 + (있으면) 아쉬운 점 0~1개를 균형 있게 담습니다.\\n\"\n","    \"- '제목:' '요약:' 같은 접두어 금지, 말줄임표(...) 금지, 반드시 한국어 종결어미로 끝냅니다.\\n\"\n","    \"- 숫자/연도는 입력에 있는 범위만 사용하고, 불필요한 연속 숫자 생성 금지.\\n\"\n","    \"- 행사명과 지역명을 1회 이상 자연스럽게 포함합니다.\\n\"\n","    \"- 출력은 한국어 문장으로만 작성합니다.\"\n",")\n","\n","def to_chat_row(r):\n","    rid   = int(r[\"contentid\"])\n","    title = str(r.get(\"event_title\", \"\") or \"\").strip()\n","    addr  = str(r.get(\"event_addr\", \"\") or \"\").strip()\n","    raw_review = str(r.get(\"cleaned_review\", \"\") or \"\")\n","    target_sum = str(r.get(\"summary\", \"\") or \"\")\n","\n","    review_ko = clean_review_to_korean(raw_review, keep_thresh=0.20, max_sents=40)\n","    target_guarded = repair_summary(target_sum, title=title, min_han_ratio=0.70)\n","\n","    if not review_ko.strip():\n","        return None\n","\n","    user_prompt = f\"행사명: {title} / 주소: {addr}\\n\\n[리뷰]\\n{review_ko}\\n\\n위 리뷰를 1~2문장으로 요약해 주세요.\"\n","\n","    return {\n","        \"id\": rid,\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": SYS_PROMPT},\n","            {\"role\": \"user\",   \"content\": user_prompt},\n","            {\"role\": \"assistant\", \"content\": target_guarded}\n","        ],\n","    }"],"metadata":{"id":"xwHG-NMLCHqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random, json\n","\n","chat_rows = []\n","dropped = 0\n","\n","for _, r in full.iterrows():\n","    row = to_chat_row(r)\n","    if row is None:\n","        dropped += 1\n","        continue\n","    if len(row[\"messages\"][1][\"content\"].split()) < 8:\n","        dropped += 1\n","        continue\n","    chat_rows.append(row)\n","\n","print(f\"총 샘플 수 후보: {len(full)}  → 사용: {len(chat_rows)}  | 드롭: {dropped}\")\n","\n","# 셔플 & 분리\n","random.seed(1337)\n","random.shuffle(chat_rows)\n","n = len(chat_rows)\n","n_val = max(50, int(n * 0.10))\n","val_rows = chat_rows[:n_val]\n","train_rows = chat_rows[n_val:]\n","\n","train_path = f\"{OUT_DIR}/train_sum_chat.CLEAN.jsonl\"\n","val_path   = f\"{OUT_DIR}/val_sum_chat.CLEAN.jsonl\"\n","\n","with open(train_path, \"w\", encoding=\"utf-8\") as f:\n","    for r in train_rows:\n","        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n","\n","with open(val_path, \"w\", encoding=\"utf-8\") as f:\n","    for r in val_rows:\n","        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"train rows: {len(train_rows)} → {train_path}\")\n","print(f\"val   rows: {len(val_rows)} → {val_path}\")\n","\n","# 간단 품질 점검\n","def avg_han(rows):\n","    ratios = []\n","    for r in rows:\n","        tgt = r[\"messages\"][2][\"content\"]\n","        ratios.append(han_ratio(tgt))\n","    return sum(ratios)/len(ratios) if ratios else 0.0\n","\n","print(\"\\n[간단 통계]\")\n","print(\"  타깃 요약 한국어비율 평균(train):\", round(avg_han(train_rows), 3))\n","print(\"  타깃 요약 한국어비율 평균(val)  :\", round(avg_han(val_rows), 3))\n","\n","print(\"\\n[프리뷰 3개]\")\n","for r in train_rows[:3]:\n","    print(\"—\")\n","    print(r[\"messages\"][1][\"content\"][:180].replace(\"\\n\",\" \"))\n","    print(\"→\", r[\"messages\"][2][\"content\"])"],"metadata":{"id":"zMWbmJhVCHoo"},"execution_count":null,"outputs":[]}]}